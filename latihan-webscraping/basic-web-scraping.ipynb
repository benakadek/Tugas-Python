{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Web Scraping with Python BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the external module BeautifulSoup4: `pip install BeautifulSoup4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from bs4 import BeautifulSoup # make sure you already install the module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, one rule to Python web scraping is make sure the you have the website owner **permission**, or use some website that is use for web scraping. You also can create your own website (contains simple html and javascript) on your local computer.\n",
    "\n",
    "Second, inspect the website HTML on your browser then determine what information you would get from the website.\n",
    "\n",
    "In the following code, I'll show you the basic web scraping with Python BeautifulSoup module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, I use this website as a test case from Python Web Scraping book\n",
    "# You can change and experiment to different website\n",
    "\n",
    "url = 'http://www.pythonscraping.com/pages/page1.html'\n",
    "html = urlopen(url)\n",
    "bs = BeautifulSoup(html.read(), 'html.parser')\n",
    "print(bs.h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://pythonscrapingthisurldoesnotexist.com' \n",
    "try:\n",
    "    html = urlopen(url)\n",
    "except HTTPError as e:\n",
    "    print('The server returned an HTTP error') # If the HTTP server is offline or error\n",
    "except URLError as e:\n",
    "    print('The server could not be found!') # If the website doesn's exist\n",
    "else:\n",
    "    print(html.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several HTML parser, BeautifulSoup support different HTML parser including:\n",
    "\n",
    "- Python’s html.parser: include in defaults Python's library, you don't need to install separately\n",
    "- lxml’s HTML parser  : third party module --> install separately (`pip install lxml`)\n",
    "- lxml’s XML parser   : third party module --> suppose already installed with `lxml`\n",
    "- html5lib parser     : third party module --> install separately (`pip install html5lib`)\n",
    "\n",
    "\n",
    "![img_1](img/beautifulsoup.jpg)\n",
    "Reference:\n",
    "https://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautifulsoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitle(url):\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        return None\n",
    "    try:\n",
    "        # Create BeautifulSoup object to read html page\n",
    "        # Option: html.parser, lxml, lxml-xml, html5lib\n",
    "        bsObj = BeautifulSoup(html.read(), 'html5lib') \n",
    "        \n",
    "        # Only read the h1 tag\n",
    "        title = bsObj.body.h1 \n",
    "    except AttributeError as e:\n",
    "        return None\n",
    "    return title\n",
    "\n",
    "\n",
    "url = 'http://www.pythonscraping.com/pages/page1.html'\n",
    "title = getTitle(url)\n",
    "if title == None:\n",
    "    print('Title could not be found')\n",
    "else:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.pythonscraping.com/pages/warandpeace.html'\n",
    "html = urlopen(url)\n",
    "bs = BeautifulSoup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the URL <'http://www.pythonscraping.com/pages/warandpeace.html'> so you can understand the following code\n",
    "nameList = bs.findAll('span', {'class': 'green'})\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print([title for title in titles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allText = bs.find_all('span', {'class':{'green', 'red'}})\n",
    "\n",
    "# Store the text in a list\n",
    "text_list = [text for text in allText]\n",
    "print(text_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = bs.find_all(id='title', class_='text')\n",
    "title_text = [text for text in allText]\n",
    "print(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Table Tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.pythonscraping.com/pages/page3.html'\n",
    "html = urlopen(url)\n",
    "bs = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "for child in bs.find('table',{'id':'giftList'}).children:\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sibling in bs.find('table', {'id':'giftList'}).tr.next_siblings:\n",
    "    print(sibling) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bs.find('img',\n",
    "              {'src':'../img/gifts/img1.jpg'})\n",
    "      .parent.previous_sibling.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "images = bs.find_all('img', {'src':re.compile('\\.\\.\\/img\\/gifts/img.*\\.jpg')})\n",
    "for image in images: \n",
    "    print(image['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.find_all(lambda tag: len(tag.attrs) == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping Particular Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.find_all(lambda tag: tag.get_text() == 'Or maybe he\\'s only resting?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs.find_all('', text='Or maybe he\\'s only resting?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
